{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import需要的套件\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "%run Dataset.ipynb\n",
    "%run Model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raed date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "#分別將 training set、validation set、testing set 用 readfile 函式讀進來\n",
    "data_dir = '../Data/food-11'\n",
    "print(\"Reading data\")\n",
    "\n",
    "train_x, train_y = readfile(os.path.join(data_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "\n",
    "val_x, val_y = readfile(os.path.join(data_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "\n",
    "test_x = readfile(os.path.join(data_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set = ImgDataset(train_x, train_y, train_transform)\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'DenseNet',\n",
       " 'GoogLeNet',\n",
       " 'Inception3',\n",
       " 'MNASNet',\n",
       " 'MobileNetV2',\n",
       " 'ResNet',\n",
       " 'ShuffleNetV2',\n",
       " 'SqueezeNet',\n",
       " 'VGG',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'utils',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/mj/.cache/torch/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 86.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True).cuda()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "    \n",
    "fc = nn.Sequential(\n",
    "    nn.Linear(512*4*4, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 11)\n",
    ")\n",
    "\n",
    "model.classifier = fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 35.30 sec(s) Train Acc: 0.590209 Loss: 0.011330 | Val Acc: 0.670262 loss: 0.008041\n",
      "[002/030] 35.53 sec(s) Train Acc: 0.746605 Loss: 0.006195 | Val Acc: 0.712828 loss: 0.006986\n",
      "[003/030] 35.54 sec(s) Train Acc: 0.793432 Loss: 0.004957 | Val Acc: 0.711953 loss: 0.007075\n",
      "[004/030] 35.28 sec(s) Train Acc: 0.820292 Loss: 0.004326 | Val Acc: 0.762682 loss: 0.006430\n",
      "[005/030] 36.06 sec(s) Train Acc: 0.833773 Loss: 0.004024 | Val Acc: 0.693294 loss: 0.007669\n",
      "[006/030] 35.51 sec(s) Train Acc: 0.831644 Loss: 0.004019 | Val Acc: 0.719825 loss: 0.007995\n",
      "[007/030] 35.49 sec(s) Train Acc: 0.859315 Loss: 0.003257 | Val Acc: 0.753353 loss: 0.006905\n",
      "[008/030] 35.58 sec(s) Train Acc: 0.860734 Loss: 0.003415 | Val Acc: 0.726531 loss: 0.007186\n",
      "[009/030] 35.28 sec(s) Train Acc: 0.872491 Loss: 0.003049 | Val Acc: 0.731778 loss: 0.006893\n",
      "[010/030] 35.31 sec(s) Train Acc: 0.903912 Loss: 0.002255 | Val Acc: 0.769679 loss: 0.006260\n",
      "[011/030] 35.28 sec(s) Train Acc: 0.903912 Loss: 0.002282 | Val Acc: 0.768513 loss: 0.006620\n",
      "[012/030] 35.34 sec(s) Train Acc: 0.919420 Loss: 0.001962 | Val Acc: 0.763265 loss: 0.006625\n",
      "[013/030] 35.32 sec(s) Train Acc: 0.892662 Loss: 0.002621 | Val Acc: 0.713120 loss: 0.009750\n",
      "[014/030] 35.32 sec(s) Train Acc: 0.848976 Loss: 0.003706 | Val Acc: 0.739359 loss: 0.007655\n",
      "[015/030] 35.94 sec(s) Train Acc: 0.889824 Loss: 0.002628 | Val Acc: 0.758601 loss: 0.006620\n",
      "[016/030] 37.06 sec(s) Train Acc: 0.909791 Loss: 0.002188 | Val Acc: 0.786880 loss: 0.005972\n",
      "[017/030] 35.83 sec(s) Train Acc: 0.928948 Loss: 0.001703 | Val Acc: 0.778134 loss: 0.005980\n",
      "[018/030] 36.70 sec(s) Train Acc: 0.933306 Loss: 0.001630 | Val Acc: 0.786589 loss: 0.006668\n",
      "[019/030] 36.91 sec(s) Train Acc: 0.951145 Loss: 0.001216 | Val Acc: 0.776385 loss: 0.006994\n",
      "[020/030] 36.02 sec(s) Train Acc: 0.916582 Loss: 0.002052 | Val Acc: 0.770554 loss: 0.006267\n",
      "[021/030] 36.96 sec(s) Train Acc: 0.933509 Loss: 0.001562 | Val Acc: 0.792711 loss: 0.006510\n",
      "[022/030] 36.30 sec(s) Train Acc: 0.941212 Loss: 0.001416 | Val Acc: 0.782216 loss: 0.007043\n",
      "[023/030] 35.59 sec(s) Train Acc: 0.952159 Loss: 0.001091 | Val Acc: 0.792128 loss: 0.006385\n",
      "[024/030] 35.58 sec(s) Train Acc: 0.961991 Loss: 0.000878 | Val Acc: 0.814869 loss: 0.005933\n",
      "[025/030] 36.13 sec(s) Train Acc: 0.974458 Loss: 0.000642 | Val Acc: 0.804956 loss: 0.006815\n",
      "[026/030] 36.54 sec(s) Train Acc: 0.945064 Loss: 0.001395 | Val Acc: 0.778134 loss: 0.007141\n",
      "[027/030] 35.28 sec(s) Train Acc: 0.963511 Loss: 0.000876 | Val Acc: 0.805831 loss: 0.006427\n",
      "[028/030] 35.33 sec(s) Train Acc: 0.974255 Loss: 0.000858 | Val Acc: 0.772886 loss: 0.008288\n",
      "[029/030] 35.31 sec(s) Train Acc: 0.903304 Loss: 0.002467 | Val Acc: 0.767347 loss: 0.006719\n",
      "[030/030] 35.26 sec(s) Train Acc: 0.957734 Loss: 0.001050 | Val Acc: 0.814286 loss: 0.005930\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        X = data[0].cuda()\n",
    "        Y = data[1].cuda()\n",
    "        optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        \n",
    "        #### What's Wrong\n",
    "        train_pred = model(X) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "        batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "            \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = Classifier().cuda()\n",
    "loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_best.eval()\n",
    "model.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model(data.cuda())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將結果寫入 csv 檔\n",
    "with open(\"../Output/predict.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
